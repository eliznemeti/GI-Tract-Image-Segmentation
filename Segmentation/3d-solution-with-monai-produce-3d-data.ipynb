{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../input/monai-v081/')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process train df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the training dataframe and display the initial dataframe\n",
    "# the way to process the df refers to:\n",
    "# https://www.kaggle.com/code/dschettler8845/uwmgit-deeplabv3-end-to-end-pipeline-tf\n",
    "\n",
    "DATA_DIR = \"/Users/elizabethnemeti/Desktop/uw-madison-gi-tract-image-segmentation\"\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# Get all training images\n",
    "all_train_images = glob(os.path.join(DATA_DIR, \"train\", \"**\", \"*.png\"), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepath_from_partial_identifier(_ident, file_list):\n",
    "    return [x for x in file_list if _ident in x][0]\n",
    "\n",
    "def df_preprocessing(df, globbed_file_list, is_test=False):\n",
    "    \"\"\" The preprocessing steps applied to get column information \"\"\"\n",
    "    print(\"Sample of globbed_file_list inside function:\\n\", globbed_file_list[:5])\n",
    "    \n",
    "    # 1. Get Case-ID as a column (str and int)\n",
    "    df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n",
    "\n",
    "    # 2. Get Day as a column\n",
    "    df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n",
    "\n",
    "    # 3. Get Slice Identifier as a column\n",
    "    df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n",
    "\n",
    "    # 4. Get full file paths for the representative scans\n",
    "    df[\"_partial_ident\"] = (globbed_file_list[0].rsplit(\"/\", 4)[0] + \"/\" +  # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n",
    "                            df[\"case_id_str\"] + \"/\" +  # .../case###/\n",
    "                            df[\"case_id_str\"] + \"_\" + df[\"day_num_str\"] +  # .../case###_day##/\n",
    "                            \"/scans/\" + df[\"slice_id\"])  # .../slice_####\n",
    "    print(\"Partial Identifiers:\\n\", df[\"_partial_ident\"].head())\n",
    "    \n",
    "    _tmp_merge_df = pd.DataFrame({\"_partial_ident\": [x.rsplit(\"_\", 4)[0] for x in globbed_file_list], \"f_path\": globbed_file_list})\n",
    "    print(\"Temporary Merge DataFrame:\\n\", _tmp_merge_df.head())\n",
    "    \n",
    "    df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n",
    "    print(\"DataFrame after merge:\\n\", df.head())\n",
    "\n",
    "    # 5. Get slice dimensions from filepath (int in pixels)\n",
    "    df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\", 4)[2]))\n",
    "    df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\", 4)[1]))\n",
    "\n",
    "    # 6. Pixel spacing from filepath (float in mm)\n",
    "    df[\"px_spacing_h\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\", 4)[3]))\n",
    "    df[\"px_spacing_w\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\", 4)[4]))\n",
    "\n",
    "    if not is_test:\n",
    "        # 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\n",
    "        l_bowel_df = df[df[\"class\"] == \"large_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\": \"lb_seg_rle\"})\n",
    "        s_bowel_df = df[df[\"class\"] == \"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\": \"sb_seg_rle\"})\n",
    "        stomach_df = df[df[\"class\"] == \"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\": \"st_seg_rle\"})\n",
    "        df = df.merge(l_bowel_df, on=\"id\", how=\"left\")\n",
    "        df = df.merge(s_bowel_df, on=\"id\", how=\"left\")\n",
    "        df = df.merge(stomach_df, on=\"id\", how=\"left\")\n",
    "        df = df.drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
    "        print(\"DataFrame after merging segmentation rows:\\n\", df.head())\n",
    "\n",
    "    # 8. Reorder columns to a new ordering (drops class and segmentation as no longer necessary)\n",
    "    new_col_order = [\"id\", \"f_path\",\n",
    "                     \"lb_seg_rle\",\n",
    "                     \"sb_seg_rle\",\n",
    "                     \"st_seg_rle\",\n",
    "                     \"slice_h\", \"slice_w\", \"px_spacing_h\",\n",
    "                     \"px_spacing_w\", \"case_id_str\",\n",
    "                     \"day_num_str\", \"slice_id\"]\n",
    "    if is_test:\n",
    "        new_col_order.insert(1, \"class\")\n",
    "    new_col_order = [_c for _c in new_col_order if _c in df.columns]\n",
    "    df = df[new_col_order]\n",
    "    print(\"Final DataFrame:\\n\", df.head())\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of globbed_file_list inside function:\n",
      " ['/Users/elizabethnemeti/Desktop/uw-madison-gi-tract-image-segmentation/train/case22/case22_day0/scans/slice_0131_266_266_1.50_1.50.png', '/Users/elizabethnemeti/Desktop/uw-madison-gi-tract-image-segmentation/train/case22/case22_day0/scans/slice_0032_266_266_1.50_1.50.png', '/Users/elizabethnemeti/Desktop/uw-madison-gi-tract-image-segmentation/train/case22/case22_day0/scans/slice_0084_266_266_1.50_1.50.png', '/Users/elizabethnemeti/Desktop/uw-madison-gi-tract-image-segmentation/train/case22/case22_day0/scans/slice_0092_266_266_1.50_1.50.png', '/Users/elizabethnemeti/Desktop/uw-madison-gi-tract-image-segmentation/train/case22/case22_day0/scans/slice_0024_266_266_1.50_1.50.png']\n",
      "Partial Identifiers:\n",
      " 0    /Users/elizabethnemeti/Desktop/uw-madison-gi-t...\n",
      "1    /Users/elizabethnemeti/Desktop/uw-madison-gi-t...\n",
      "2    /Users/elizabethnemeti/Desktop/uw-madison-gi-t...\n",
      "3    /Users/elizabethnemeti/Desktop/uw-madison-gi-t...\n",
      "4    /Users/elizabethnemeti/Desktop/uw-madison-gi-t...\n",
      "Name: _partial_ident, dtype: object\n",
      "Temporary Merge DataFrame:\n",
      "                                       _partial_ident  \\\n",
      "0  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...   \n",
      "1  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...   \n",
      "2  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...   \n",
      "3  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...   \n",
      "4  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...   \n",
      "\n",
      "                                              f_path  \n",
      "0  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...  \n",
      "1  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...  \n",
      "2  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...  \n",
      "3  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...  \n",
      "4  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...  \n",
      "DataFrame after merge:\n",
      "                          id  \\\n",
      "0  case123_day20_slice_0001   \n",
      "1  case123_day20_slice_0001   \n",
      "2  case123_day20_slice_0001   \n",
      "3  case123_day20_slice_0002   \n",
      "4  case123_day20_slice_0002   \n",
      "\n",
      "                                            f_path_x lb_seg_rle sb_seg_rle  \\\n",
      "0  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...        NaN        NaN   \n",
      "1  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...        NaN        NaN   \n",
      "2  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...        NaN        NaN   \n",
      "3  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...        NaN        NaN   \n",
      "4  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...        NaN        NaN   \n",
      "\n",
      "  st_seg_rle  slice_h  slice_w  px_spacing_h  px_spacing_w case_id_str  \\\n",
      "0        NaN      266      266           1.5           1.5     case123   \n",
      "1        NaN      266      266           1.5           1.5     case123   \n",
      "2        NaN      266      266           1.5           1.5     case123   \n",
      "3        NaN      266      266           1.5           1.5     case123   \n",
      "4        NaN      266      266           1.5           1.5     case123   \n",
      "\n",
      "  day_num_str    slice_id  fold  \\\n",
      "0       day20  slice_0001     3   \n",
      "1       day20  slice_0001     3   \n",
      "2       day20  slice_0001     3   \n",
      "3       day20  slice_0002     3   \n",
      "4       day20  slice_0002     3   \n",
      "\n",
      "                                            f_path_y  \\\n",
      "0  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...   \n",
      "1  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...   \n",
      "2  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...   \n",
      "3  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...   \n",
      "4  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...   \n",
      "\n",
      "                                              f_path  \n",
      "0  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...  \n",
      "1  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...  \n",
      "2  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...  \n",
      "3  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...  \n",
      "4  /Users/elizabethnemeti/Desktop/uw-madison-gi-t...  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/UNETENV/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m splits \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/elizabethnemeti/Desktop/uwdatapreprocessing/splits.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Merge with train_df\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_train_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Ensure 'f_path' column exists after preprocessing\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_path\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m train_df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "Cell \u001b[0;32mIn[57], line 40\u001b[0m, in \u001b[0;36mdf_preprocessing\u001b[0;34m(df, globbed_file_list, is_test)\u001b[0m\n\u001b[1;32m     36\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpx_spacing_w\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mfloat\u001b[39m(x[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m)[\u001b[38;5;241m4\u001b[39m]))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_test:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     l_bowel_df \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge_bowel\u001b[39m\u001b[38;5;124m\"\u001b[39m][[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlb_seg_rle\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     41\u001b[0m     s_bowel_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall_bowel\u001b[39m\u001b[38;5;124m\"\u001b[39m][[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msb_seg_rle\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     42\u001b[0m     stomach_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstomach\u001b[39m\u001b[38;5;124m\"\u001b[39m][[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mst_seg_rle\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/UNETENV/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/UNETENV/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "# I use the same data splits as AWSAF does\n",
    "# the splits.csv refers to:\n",
    "# https://www.kaggle.com/code/awsaf49/uwmgi-unet-train-pytorch/\n",
    "\n",
    "# Load splits.csv\n",
    "splits = pd.read_csv(\"/Users/elizabethnemeti/Desktop/uwdatapreprocessing/splits.csv\")\n",
    "\n",
    "# Merge with train_df\n",
    "train_df = df_preprocessing(train_df, all_train_images)\n",
    "\n",
    "# Ensure 'f_path' column exists after preprocessing\n",
    "if 'f_path' not in train_df.columns:\n",
    "    raise KeyError(\"'f_path' column not found in train_df after preprocessing\")\n",
    "\n",
    "train_df = train_df.merge(splits, on=\"id\")\n",
    "train_df[\"fold\"] = train_df[\"fold\"].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "# modified from: https://www.kaggle.com/inversion/run-length-decoding-quick-start\n",
    "def rle_decode(mask_rle, shape, color=1):\n",
    "    \"\"\" TBD\n",
    "    \n",
    "    Args:\n",
    "        mask_rle (str): run-length as string formated (start length)\n",
    "        shape (tuple of ints): (height,width) of array to return \n",
    "    \n",
    "    Returns: \n",
    "        Mask (np.array)\n",
    "            - 1 indicating mask\n",
    "            - 0 indicating background\n",
    "\n",
    "    \"\"\"\n",
    "    # Split the string by space, then convert it into a integer array\n",
    "    s = np.array(mask_rle.split(), dtype=int)\n",
    "\n",
    "    # Every even value is the start, every odd value is the \"run\" length\n",
    "    starts = s[0::2] - 1\n",
    "    lengths = s[1::2]\n",
    "    ends = starts + lengths\n",
    "\n",
    "    # The image image is actually flattened since RLE is a 1D \"run\"\n",
    "    if len(shape)==3:\n",
    "        h, w, d = shape\n",
    "        img = np.zeros((h * w, d), dtype=np.float32)\n",
    "    else:\n",
    "        h, w = shape\n",
    "        img = np.zeros((h * w,), dtype=np.float32)\n",
    "\n",
    "    # The color here is actually just any integer you want!\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = color\n",
    "        \n",
    "    # Don't forget to change the image back to the original shape\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def load_img_mask(l):\n",
    "    img_data = loader(l.f_path)\n",
    "    img_h, img_w = img_data[0].shape\n",
    "    shape = (l.slice_h, l.slice_w)\n",
    "    assert shape == (img_h, img_w)\n",
    "    wh_shape = (img_w, img_h)\n",
    "    if pd.isna(l.lb_seg_rle):\n",
    "        lb_mask = np.zeros(wh_shape)\n",
    "    else:\n",
    "        lb_mask = rle_decode(l.lb_seg_rle, wh_shape)\n",
    "        \n",
    "    if pd.isna(l.sb_seg_rle):\n",
    "        sb_mask = np.zeros(wh_shape)\n",
    "    else:\n",
    "        sb_mask = rle_decode(l.sb_seg_rle, wh_shape)\n",
    "        \n",
    "    if pd.isna(l.st_seg_rle):\n",
    "        st_mask = np.zeros(wh_shape)\n",
    "    else:\n",
    "        st_mask = rle_decode(l.st_seg_rle, wh_shape)\n",
    "    \n",
    "    all_mask = np.stack([lb_mask, sb_mask, st_mask], axis=0).astype(np.uint8)\n",
    "    # multiclass mask,\n",
    "    mask_arr = st_mask*3\n",
    "    mask_arr = np.where(sb_mask==1, 2, mask_arr)\n",
    "    mask_arr = np.where(lb_mask==1, 1, mask_arr)\n",
    "    \n",
    "    return img_data[0], all_mask, mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_mask(l):\n",
    "    img_data = loader(l[\"f_path\"])  # Changed from l.f_path to l[\"f_path\"]\n",
    "    img_h, img_w = img_data[0].shape\n",
    "    shape = (l[\"slice_h\"], l[\"slice_w\"])  # Changed from l.slice_h and l.slice_w to l[\"slice_h\"] and l[\"slice_w\"]\n",
    "    assert shape == (img_h, img_w)\n",
    "    wh_shape = (img_w, img_h)\n",
    "    lb_mask = np.zeros(wh_shape) if pd.isna(l[\"lb_seg_rle\"]) else rle_decode(l[\"lb_seg_rle\"], wh_shape)  # Changed from l.lb_seg_rle to l[\"lb_seg_rle\"]\n",
    "    sb_mask = np.zeros(wh_shape) if pd.isna(l[\"sb_seg_rle\"]) else rle_decode(l[\"sb_seg_rle\"], wh_shape)  # Changed from l.sb_seg_rle to l[\"sb_seg_rle\"]\n",
    "    st_mask = np.zeros(wh_shape) if pd.isna(l[\"st_seg_rle\"]) else rle_decode(l[\"st_seg_rle\"], wh_shape)  # Changed from l.st_seg_rle to l[\"st_seg_rle\"]\n",
    "    all_mask = np.stack([lb_mask, sb_mask, st_mask], axis=0).astype(np.uint8)\n",
    "    mask_arr = st_mask * 3\n",
    "    mask_arr = np.where(sb_mask == 1, 2, mask_arr)\n",
    "    mask_arr = np.where(lb_mask == 1, 1, mask_arr)\n",
    "    return img_data[0], all_mask, mask_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import LoadImage\n",
    "from monai.data import NibabelWriter\n",
    "\n",
    "loader = LoadImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 3D images and masks and save to Nibabel format\n",
    "\n",
    "The reason to use Nibabel format is that spacing information can be added into it, it can be used with some MONAI transforms\n",
    "Both multi-label masks (for validation) and multi-class masks (for training) are produced, since I felt hard to tune a multi-label 3D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'f_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/UNETENV/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'f_path'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_slices):\n\u001b[1;32m     12\u001b[0m     slc \u001b[38;5;241m=\u001b[39m group_df\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[0;32m---> 13\u001b[0m     slc_img, slc_mask, slc_multiclass_mask \u001b[38;5;241m=\u001b[39m \u001b[43mload_img_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     case_3d_img\u001b[38;5;241m.\u001b[39mappend(slc_img)\n\u001b[1;32m     15\u001b[0m     case_3d_mask\u001b[38;5;241m.\u001b[39mappend(slc_mask)\n",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m, in \u001b[0;36mload_img_mask\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_img_mask\u001b[39m(l):\n\u001b[0;32m----> 2\u001b[0m     img_data \u001b[38;5;241m=\u001b[39m loader(\u001b[43ml\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Changed from l.f_path to l[\"f_path\"]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     img_h, img_w \u001b[38;5;241m=\u001b[39m img_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m     shape \u001b[38;5;241m=\u001b[39m (l[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice_h\u001b[39m\u001b[38;5;124m\"\u001b[39m], l[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice_w\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# Changed from l.slice_h and l.slice_w to l[\"slice_h\"] and l[\"slice_w\"]\u001b[39;00m\n",
      "File \u001b[0;32m~/UNETENV/lib/python3.11/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/UNETENV/lib/python3.11/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/UNETENV/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'f_path'"
     ]
    }
   ],
   "source": [
    "data_3d_info = []\n",
    "ct = 0\n",
    "for group in train_df.groupby([\"case_id_str\", \"day_num_str\"]):\n",
    "\n",
    "    case_3d_img, case_3d_mask, case_3d_mask_multiclass = [], [], []\n",
    "    \n",
    "    case_id_str, day_num_str = group[0]\n",
    "    group_id = case_id_str + \"_\" + day_num_str\n",
    "    group_df = group[1].sort_values(\"slice_id\", ascending=True)\n",
    "    n_slices = group_df.shape[0]\n",
    "    for idx in range(n_slices):\n",
    "        slc = group_df.iloc[idx]\n",
    "        slc_img, slc_mask, slc_multiclass_mask = load_img_mask(slc)\n",
    "        case_3d_img.append(slc_img)\n",
    "        case_3d_mask.append(slc_mask)\n",
    "        case_3d_mask_multiclass.append(slc_multiclass_mask)\n",
    "    \n",
    "    case_3d_img = np.stack(case_3d_img, axis=-1)\n",
    "    case_3d_mask = np.stack(case_3d_mask, axis=-1)\n",
    "    case_3d_mask = np.transpose(case_3d_mask, [2, 1, 3, 0]) # c w h d to h w d c\n",
    "    case_3d_mask_multiclass = np.stack(case_3d_mask_multiclass, axis=-1)\n",
    "    case_3d_mask_multiclass = np.transpose(case_3d_mask_multiclass, [1, 0, 2]) # w h d to h w d\n",
    "\n",
    "    assert np.all(case_3d_mask.astype(np.uint8) == case_3d_mask)\n",
    "    case_3d_mask = case_3d_mask.astype(np.uint8)\n",
    "\n",
    "    if case_3d_mask.shape[:-1] != case_3d_img.shape:\n",
    "        print(\"shape not match on group: \", group_id)\n",
    "\n",
    "    group_spacing = group[1][[\"px_spacing_h\"]].values[0][0]\n",
    "\n",
    "    group_affine = np.eye(4) * group_spacing\n",
    "    # Update: https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/319053\n",
    "    # all z-axis spacing is 3\n",
    "    group_affine[-2][-2] = 3.0\n",
    "    group_affine[-1][-1] = 1.0\n",
    "    group_fold = group[1][[\"fold\"]].values[0][0]\n",
    "\n",
    "    group_root_dir = os.path.join(output_dir, \"train\", case_id_str, group_id)\n",
    "    os.makedirs(group_root_dir)\n",
    "    # write image\n",
    "    writer = NibabelWriter()\n",
    "    writer.set_data_array(case_3d_img, channel_dim=None)\n",
    "    writer.set_metadata({\"affine\": group_affine, \"original_affine\": group_affine, \"dtype\": np.int16})\n",
    "    writer.write(f\"{group_root_dir}/{group_id}_image.nii.gz\", verbose=False)\n",
    "\n",
    "    # write mask\n",
    "    writer = NibabelWriter()\n",
    "    writer.set_data_array(case_3d_mask, channel_dim=-1)\n",
    "    writer.set_metadata({\"affine\": group_affine, \"original_affine\": group_affine, \"dtype\": np.uint8})\n",
    "    writer.write(f\"{group_root_dir}/{group_id}_mask.nii.gz\", verbose=False)\n",
    "    \n",
    "    # write mask multiclass\n",
    "    writer = NibabelWriter()\n",
    "    writer.set_data_array(case_3d_mask_multiclass, channel_dim=None)\n",
    "    writer.set_metadata({\"affine\": group_affine, \"original_affine\": group_affine, \"dtype\": np.uint8})\n",
    "    writer.write(f\"{group_root_dir}/{group_id}_mask_multiclass.nii.gz\", verbose=False)\n",
    "\n",
    "    data_3d_info.append({\n",
    "        \"id\": group_id,\n",
    "        \"fold\": group_fold,\n",
    "        \"image_path\": f\"{group_root_dir}/{group_id}_image.nii.gz\",\n",
    "        \"mask_path\": f\"{group_root_dir}/{group_id}_mask.nii.gz\",\n",
    "        \"mask_multiclass_path\": f\"{group_root_dir}/{group_id}_mask_multiclass.nii.gz\",\n",
    "    })\n",
    "\n",
    "    ct += 1\n",
    "    print(\"finish: \", ct, \" shape: \", case_3d_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3d_info = pd.DataFrame(data_3d_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3d_info.to_csv(\"data_3d_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(5):\n",
    "    train_data, val_data = [], []\n",
    "    train_df = data_3d_info[data_3d_info[\"fold\"] != fold]\n",
    "    val_df = data_3d_info[data_3d_info[\"fold\"] == fold]\n",
    "    \n",
    "    for line in train_df.values:\n",
    "        train_data.append({\"image\": line[2], \"mask\": line[3], \"mask_multiclass\": line[4], \"id\": line[0]})\n",
    "    for line in val_df.values:\n",
    "        val_data.append({\"image\": line[2], \"mask\": line[3], \"mask_multiclass\": line[4], \"id\": line[0]})\n",
    "\n",
    "    all_data = {\"train\": train_data, \"val\": val_data}\n",
    "    \n",
    "    with open(f\"dataset_3d_fold_{fold}.json\", 'w') as f:\n",
    "        json.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 3495119,
     "sourceId": 27923,
     "sourceType": "competition"
    },
    {
     "datasetId": 2118343,
     "sourceId": 3520621,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2186316,
     "sourceId": 3651238,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30191,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (UNETENV)",
   "language": "python",
   "name": "unetenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
