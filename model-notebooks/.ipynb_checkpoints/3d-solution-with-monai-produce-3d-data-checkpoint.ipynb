{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":27923,"databundleVersionId":3495119,"sourceType":"competition"},{"sourceId":3520621,"sourceType":"datasetVersion","datasetId":2118343},{"sourceId":3651238,"sourceType":"datasetVersion","datasetId":2186316}],"dockerImageVersionId":30191,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\n\nsys.path.append('../input/monai-v081/')\n\nimport pandas as pd\nimport json\nimport torch\nimport os\nimport numpy as np\nfrom glob import glob","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:59:21.928319Z","iopub.execute_input":"2022-05-24T03:59:21.929162Z","iopub.status.idle":"2022-05-24T03:59:24.033485Z","shell.execute_reply.started":"2022-05-24T03:59:21.929087Z","shell.execute_reply":"2022-05-24T03:59:24.03206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Process train df","metadata":{}},{"cell_type":"code","source":"# Open the training dataframe and display the initial dataframe\n# the way to process the df refers to:\n# https://www.kaggle.com/code/dschettler8845/uwmgit-deeplabv3-end-to-end-pipeline-tf\n\nDATA_DIR = \"../input/uw-madison-gi-tract-image-segmentation/\"\n\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\ntrain_df = pd.read_csv(TRAIN_CSV)\n\n# Get all training images\nall_train_images = glob(os.path.join(DATA_DIR, \"train\", \"**\", \"*.png\"), recursive=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:59:24.986387Z","iopub.execute_input":"2022-05-24T03:59:24.986744Z","iopub.status.idle":"2022-05-24T03:59:29.75284Z","shell.execute_reply.started":"2022-05-24T03:59:24.986701Z","shell.execute_reply":"2022-05-24T03:59:29.751747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_filepath_from_partial_identifier(_ident, file_list):\n    return [x for x in file_list if _ident in x][0]\n\ndef df_preprocessing(df, globbed_file_list, is_test=False):\n    \"\"\" The preprocessing steps applied to get column information \"\"\"\n    # 1. Get Case-ID as a column (str and int)\n    df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n\n    # 2. Get Day as a column\n    df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n\n    # 3. Get Slice Identifier as a column\n    df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n\n    # 4. Get full file paths for the representative scans\n    df[\"_partial_ident\"] = (globbed_file_list[0].rsplit(\"/\", 4)[0]+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n                           df[\"case_id_str\"]+\"/\"+ # .../case###/\n                           df[\"case_id_str\"]+\"_\"+df[\"day_num_str\"]+ # .../case###_day##/\n                           \"/scans/\"+df[\"slice_id\"]) # .../slice_#### \n    _tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in globbed_file_list], \"f_path\":globbed_file_list})\n    df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n\n    # 5. Get slice dimensions from filepath (int in pixels)\n    df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n    df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n\n    # 6. Pixel spacing from filepath (float in mm)\n    df[\"px_spacing_h\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[3]))\n    df[\"px_spacing_w\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[4]))\n\n    if not is_test:\n        # 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\n        l_bowel_df = df[df[\"class\"]==\"large_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"lb_seg_rle\"})\n        s_bowel_df = df[df[\"class\"]==\"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"sb_seg_rle\"})\n        stomach_df = df[df[\"class\"]==\"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"st_seg_rle\"})\n        df = df.merge(l_bowel_df, on=\"id\", how=\"left\")\n        df = df.merge(s_bowel_df, on=\"id\", how=\"left\")\n        df = df.merge(stomach_df, on=\"id\", how=\"left\")\n        df = df.drop_duplicates(subset=[\"id\",]).reset_index(drop=True)\n\n    # 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\n    new_col_order = [\"id\", \"f_path\",\n                     \"lb_seg_rle\",\n                     \"sb_seg_rle\", \n                     \"st_seg_rle\",\n                     \"slice_h\", \"slice_w\", \"px_spacing_h\", \n                     \"px_spacing_w\", \"case_id_str\", \n                     \"day_num_str\", \"slice_id\",]\n    if is_test: new_col_order.insert(1, \"class\")\n    new_col_order = [_c for _c in new_col_order if _c in df.columns]\n    df = df[new_col_order]\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:59:29.755325Z","iopub.execute_input":"2022-05-24T03:59:29.75559Z","iopub.status.idle":"2022-05-24T03:59:29.778014Z","shell.execute_reply.started":"2022-05-24T03:59:29.75556Z","shell.execute_reply":"2022-05-24T03:59:29.77722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = df_preprocessing(train_df, all_train_images)\n# I use the same data splits as AWSAF does\n# the splits.csv refers to:\n# https://www.kaggle.com/code/awsaf49/uwmgi-unet-train-pytorch/\nsplits = pd.read_csv(\"../input/uwdatapreprocessing/splits.csv\")\n\ntrain_df = train_df.merge(splits, on=\"id\")\ntrain_df[\"fold\"] = train_df[\"fold\"].astype(np.uint8)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:59:29.779376Z","iopub.execute_input":"2022-05-24T03:59:29.779812Z","iopub.status.idle":"2022-05-24T03:59:31.642437Z","shell.execute_reply.started":"2022-05-24T03:59:29.779767Z","shell.execute_reply":"2022-05-24T03:59:31.641485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n# modified from: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns: \n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    # Split the string by space, then convert it into a integer array\n    s = np.array(mask_rle.split(), dtype=int)\n\n    # Every even value is the start, every odd value is the \"run\" length\n    starts = s[0::2] - 1\n    lengths = s[1::2]\n    ends = starts + lengths\n\n    # The image image is actually flattened since RLE is a 1D \"run\"\n    if len(shape)==3:\n        h, w, d = shape\n        img = np.zeros((h * w, d), dtype=np.float32)\n    else:\n        h, w = shape\n        img = np.zeros((h * w,), dtype=np.float32)\n\n    # The color here is actually just any integer you want!\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n        \n    # Don't forget to change the image back to the original shape\n    return img.reshape(shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:59:31.644663Z","iopub.execute_input":"2022-05-24T03:59:31.644932Z","iopub.status.idle":"2022-05-24T03:59:31.6556Z","shell.execute_reply.started":"2022-05-24T03:59:31.644899Z","shell.execute_reply":"2022-05-24T03:59:31.654329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_img_mask(l):\n    img_data = loader(l.f_path)\n    img_h, img_w = img_data[0].shape\n    shape = (l.slice_h, l.slice_w)\n    assert shape == (img_h, img_w)\n    wh_shape = (img_w, img_h)\n    if pd.isna(l.lb_seg_rle):\n        lb_mask = np.zeros(wh_shape)\n    else:\n        lb_mask = rle_decode(l.lb_seg_rle, wh_shape)\n        \n    if pd.isna(l.sb_seg_rle):\n        sb_mask = np.zeros(wh_shape)\n    else:\n        sb_mask = rle_decode(l.sb_seg_rle, wh_shape)\n        \n    if pd.isna(l.st_seg_rle):\n        st_mask = np.zeros(wh_shape)\n    else:\n        st_mask = rle_decode(l.st_seg_rle, wh_shape)\n    \n    all_mask = np.stack([lb_mask, sb_mask, st_mask], axis=0).astype(np.uint8)\n    # multiclass mask,\n    mask_arr = st_mask*3\n    mask_arr = np.where(sb_mask==1, 2, mask_arr)\n    mask_arr = np.where(lb_mask==1, 1, mask_arr)\n    \n    return img_data[0], all_mask, mask_arr","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:59:31.657085Z","iopub.execute_input":"2022-05-24T03:59:31.657418Z","iopub.status.idle":"2022-05-24T03:59:31.675573Z","shell.execute_reply.started":"2022-05-24T03:59:31.657378Z","shell.execute_reply":"2022-05-24T03:59:31.674373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from monai.transforms import LoadImage\nfrom monai.data import NibabelWriter\n\nloader = LoadImage()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:59:31.677005Z","iopub.execute_input":"2022-05-24T03:59:31.677819Z","iopub.status.idle":"2022-05-24T03:59:43.737373Z","shell.execute_reply.started":"2022-05-24T03:59:31.677774Z","shell.execute_reply":"2022-05-24T03:59:43.736113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load 3D images and masks and save to Nibabel format\n\nThe reason to use Nibabel format is that spacing information can be added into it, it can be used with some MONAI transforms\nBoth multi-label masks (for validation) and multi-class masks (for training) are produced, since I felt hard to tune a multi-label 3D model.","metadata":{}},{"cell_type":"code","source":"output_dir = \"/kaggle/working/\"","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:59:43.740143Z","iopub.execute_input":"2022-05-24T03:59:43.740472Z","iopub.status.idle":"2022-05-24T03:59:43.745619Z","shell.execute_reply.started":"2022-05-24T03:59:43.740434Z","shell.execute_reply":"2022-05-24T03:59:43.744578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_3d_info = []\nct = 0\nfor group in train_df.groupby([\"case_id_str\", \"day_num_str\"]):\n\n    case_3d_img, case_3d_mask, case_3d_mask_multiclass = [], [], []\n    \n    case_id_str, day_num_str = group[0]\n    group_id = case_id_str + \"_\" + day_num_str\n    group_df = group[1].sort_values(\"slice_id\", ascending=True)\n    n_slices = group_df.shape[0]\n    for idx in range(n_slices):\n        slc = group_df.iloc[idx]\n        slc_img, slc_mask, slc_multiclass_mask = load_img_mask(slc)\n        case_3d_img.append(slc_img)\n        case_3d_mask.append(slc_mask)\n        case_3d_mask_multiclass.append(slc_multiclass_mask)\n    \n    case_3d_img = np.stack(case_3d_img, axis=-1)\n    case_3d_mask = np.stack(case_3d_mask, axis=-1)\n    case_3d_mask = np.transpose(case_3d_mask, [2, 1, 3, 0]) # c w h d to h w d c\n    case_3d_mask_multiclass = np.stack(case_3d_mask_multiclass, axis=-1)\n    case_3d_mask_multiclass = np.transpose(case_3d_mask_multiclass, [1, 0, 2]) # w h d to h w d\n\n    assert np.all(case_3d_mask.astype(np.uint8) == case_3d_mask)\n    case_3d_mask = case_3d_mask.astype(np.uint8)\n\n    if case_3d_mask.shape[:-1] != case_3d_img.shape:\n        print(\"shape not match on group: \", group_id)\n\n    group_spacing = group[1][[\"px_spacing_h\"]].values[0][0]\n\n    group_affine = np.eye(4) * group_spacing\n    # Update: https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation/discussion/319053\n    # all z-axis spacing is 3\n    group_affine[-2][-2] = 3.0\n    group_affine[-1][-1] = 1.0\n    group_fold = group[1][[\"fold\"]].values[0][0]\n\n    group_root_dir = os.path.join(output_dir, \"train\", case_id_str, group_id)\n    os.makedirs(group_root_dir)\n    # write image\n    writer = NibabelWriter()\n    writer.set_data_array(case_3d_img, channel_dim=None)\n    writer.set_metadata({\"affine\": group_affine, \"original_affine\": group_affine, \"dtype\": np.int16})\n    writer.write(f\"{group_root_dir}/{group_id}_image.nii.gz\", verbose=False)\n\n    # write mask\n    writer = NibabelWriter()\n    writer.set_data_array(case_3d_mask, channel_dim=-1)\n    writer.set_metadata({\"affine\": group_affine, \"original_affine\": group_affine, \"dtype\": np.uint8})\n    writer.write(f\"{group_root_dir}/{group_id}_mask.nii.gz\", verbose=False)\n    \n    # write mask multiclass\n    writer = NibabelWriter()\n    writer.set_data_array(case_3d_mask_multiclass, channel_dim=None)\n    writer.set_metadata({\"affine\": group_affine, \"original_affine\": group_affine, \"dtype\": np.uint8})\n    writer.write(f\"{group_root_dir}/{group_id}_mask_multiclass.nii.gz\", verbose=False)\n\n    data_3d_info.append({\n        \"id\": group_id,\n        \"fold\": group_fold,\n        \"image_path\": f\"{group_root_dir}/{group_id}_image.nii.gz\",\n        \"mask_path\": f\"{group_root_dir}/{group_id}_mask.nii.gz\",\n        \"mask_multiclass_path\": f\"{group_root_dir}/{group_id}_mask_multiclass.nii.gz\",\n    })\n\n    ct += 1\n    print(\"finish: \", ct, \" shape: \", case_3d_mask.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T03:59:43.747671Z","iopub.execute_input":"2022-05-24T03:59:43.747986Z","iopub.status.idle":"2022-05-24T03:59:48.20191Z","shell.execute_reply.started":"2022-05-24T03:59:43.747947Z","shell.execute_reply":"2022-05-24T03:59:48.200752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_3d_info = pd.DataFrame(data_3d_info)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_3d_info.to_csv(\"data_3d_info.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(5):\n    train_data, val_data = [], []\n    train_df = data_3d_info[data_3d_info[\"fold\"] != fold]\n    val_df = data_3d_info[data_3d_info[\"fold\"] == fold]\n    \n    for line in train_df.values:\n        train_data.append({\"image\": line[2], \"mask\": line[3], \"mask_multiclass\": line[4], \"id\": line[0]})\n    for line in val_df.values:\n        val_data.append({\"image\": line[2], \"mask\": line[3], \"mask_multiclass\": line[4], \"id\": line[0]})\n\n    all_data = {\"train\": train_data, \"val\": val_data}\n    \n    with open(f\"dataset_3d_fold_{fold}.json\", 'w') as f:\n        json.dump(all_data, f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}